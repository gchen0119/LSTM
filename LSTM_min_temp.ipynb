{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Daily Minimum Temperature Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes for this project is modified from [Thushan Ganegedara's Datacamp tutorial](https://www.datacamp.com/community/tutorials/lstm-python-stock-market)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background for LSTM\n",
    "The long short-term memory (LSTM) unit is an improved version of gated recurrent unit (GRU), which tries to resolve the [vanishing gradient problem](http://neuralnetworksanddeeplearning.com/chap5.html) and keep the long term \"memory\" activated.\n",
    "\n",
    "![alt text](LSTM_rnn.png \"LSTM rnn\")\n",
    "![alt text](LSTM.png \"LSTM cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Picture summary ([Adrew Ng's lecture](https://www.coursera.org/specializations/deep-learning)):\n",
    "\n",
    "> Four parallel layers of interacting networks.\n",
    "\n",
    "> The weighted sum of the input and previous hidden output gets transformed by the activation functions (sigma (0 to 1) and tanh (-1 to 1)).  The `dot`/`add` signs represent elementwise `multiplication`/`addition`. \n",
    "\n",
    "> The cell state c[t] essentially stores the memory, which comes from multiplying the previous cell state to the `forgetness` (0 to 1). This essentially forgets/keeps the previous cell state if forgetness is near zero/one. \n",
    "\n",
    "> The activation/hidden state (last equation) is composed of current state `filter` (0 to 1) by the current cell state activation (-1 to 1) with previous memory. The hidden state connects to the ouput with softmax layer for prediction.\n",
    "\n",
    "> Notice the [`peephole connection`](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf), in other variation of LSTM, is not shown in the figure. It is done by adding another weighted sum of previous cell state c[t-1] to the forget and update gate, and add c[t] to the output gate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "Generate batches of sequenced data for the input and output data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DataGeneratorSeq(object):\n",
    "    # prices: total training time-series data\n",
    "    # batch_size: the length of a batch/sequence\n",
    "    # num_unroll: sampled number of batches/sequences\n",
    "    # segments: total number of segments in a series that is divided by the batch_size\n",
    "    \n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        print(self._cursor)\n",
    "        print(self._segments)\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and generate the data using the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 182, 364, 546, 728, 910, 1092, 1274, 1456, 1638, 1820, 2002, 2184, 2366, 2548, 2730, 2912, 3094]\n",
      "182\n",
      "1981-01-01    20.700001\n",
      "1981-01-02    17.900000\n",
      "1981-01-03    18.799999\n",
      "1981-01-04    14.600000\n",
      "1981-01-05    15.800000\n",
      "1981-01-06    15.800000\n",
      "1981-01-07    15.800000\n",
      "1981-01-08    17.400000\n",
      "1981-01-09    21.799999\n",
      "1981-01-10    20.000000\n",
      "1981-01-11    16.200001\n",
      "1981-01-12    13.300000\n",
      "1981-01-13    16.700001\n",
      "1981-01-14    21.500000\n",
      "1981-01-15    25.000000\n",
      "1981-01-16    20.700001\n",
      "1981-01-17    20.600000\n",
      "1981-01-18    24.799999\n",
      "1981-01-19    17.700001\n",
      "1981-01-20    15.500000\n",
      "1981-01-21    18.200001\n",
      "1981-01-22    12.100000\n",
      "1981-01-23    14.400000\n",
      "1981-01-24    16.000000\n",
      "1981-01-25    16.500000\n",
      "Freq: D, Name: mint, dtype: float32\n",
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:  [20.7 10.  17.4  4.2 17.7  5.5 16.1  7.8 12.   8.  13.3  6.9 10.5  7.\n",
      " 11.2 10.6 15.2  5. ]\n",
      "\n",
      "\tOutput: [15.8  7.4 17.4  4.2 10.9  9.5 19.5  2.6 12.  10.4 16.3  6.9 10.5  7.5\n",
      " 12.1  8.1  9.5  5.3]\n",
      "\n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:  [17.9  6.5 17.   1.6 16.3  8.5 20.4  9.1 12.6 10.2 11.5  7.7 14.7  8.9\n",
      " 12.1  8.1 17.3  4.7]\n",
      "\n",
      "\tOutput: [15.8  7.4 15.2  3.4 16.3  9.5 18.   2.4 16.4 10.4 11.5  3.9 14.6  8.\n",
      " 14.3  6.7 12.9  2.3]\n",
      "\n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:  [18.8  7.  15.   2.6 18.4  9.8 18.   9.4 16.  10.6 10.8  8.  14.6  9.3\n",
      " 12.7  6.7 19.8  5.3]\n",
      "\n",
      "\tOutput: [14.6  8.3 13.5  3.9 15.   9.8 19.5  2.4 14.  10.7 12.   8.  11.7  9.3\n",
      " 12.7  6.7 15.8  4.5]\n",
      "\n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:  [14.6  7.4 13.5  3.4 15.   9.5 19.5  7.8 16.4 10.4 12.   3.9 14.2  6.8\n",
      " 16.2  8.  15.8  4.5]\n",
      "\n",
      "\tOutput: [15.8  8.1 12.5  5.3 14.8  8.5 11.   7.8 15.2 10.7 13.8  2.8 11.7  8.\n",
      " 16.2  9.4 15.8  2.3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "series = pd.read_csv('~/Downloads/daily-minimum-temperatures-in-me.csv', error_bad_lines=False)\n",
    "series.rename(columns={'Daily minimum temperatures in Melbourne, Australia, 1981-1990':'mint'},inplace=True) # rename minimum temp to 'mint'\n",
    "y = pd.to_numeric(series[\"mint\"],downcast='float')\n",
    "y.index = pd.DatetimeIndex(start='1981-01-01',end='1990-12-31',freq='d')\n",
    "freq=365 # sampling freq\n",
    "train, valid = y[:freq*9], y[freq*9:]\n",
    "train.index, valid.index = y.index[:freq*9], y.index[freq*9:]\n",
    "\n",
    "dg = DataGeneratorSeq(train,18,4)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "print(dg._prices.head(25))\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Why use LSTM? (paper collection)](http://people.idsia.ch/~juergen/rnn.html)\n",
    "* [LSTM for stock prediction, referenece project](https://www.datacamp.com/community/tutorials/lstm-python-stock-market)\n",
    "* [vanishing gradient problem explained](http://neuralnetworksanddeeplearning.com/chap5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
